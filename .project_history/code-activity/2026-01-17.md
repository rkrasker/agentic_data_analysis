# Code Activity Log: 2026-01-17

**Session:** Claude Code (Opus 4.5)
**Focus:** Collision sampling fix + quality tier filtering for resolver generation

---

## Summary

Implemented fixes for two issues causing resolver generation to produce trivial output:

1. **Collision sampling bug** - Sampler was pulling from ALL soldiers instead of only those in colliding sub-units, allowing LLM to learn trivial rules based on non-overlapping designators.

2. **Quality tier filtering** - Added filtering to skew vocabulary discovery toward lower-quality records and exclude high-quality records from differentiator/pattern stages, forcing LLM to find subtle vocabulary signals rather than relying on explicit unit identifiers.

---

## Files Modified

| File | Changes |
|------|---------|
| `src/strategies/resolver/generator/sampling.py` | Added `_filter_to_collision()` helper, modified `sample_collisions()` to use filter, added logging and fallback handling |
| `src/strategies/resolver/generator/llm_phases.py` | Added `_filter_records_by_quality()` helper, updated `discover_vocabulary()`, `discover_patterns()`, and `mine_exclusions()` to use quality filtering |
| `instructions/active/002_collision-sampling-synthetic-fix.md` | Marked Phase 1 tasks as complete |

---

## Implementation Details

### 1. Collision-Scoped Sampling (`sampling.py`)

Added `_filter_to_collision()` helper (lines 235-276):

```python
def _filter_to_collision(
    soldiers: List[str],
    train_df: pd.DataFrame,
    collision_levels: List[Tuple[str, str]],
) -> List[str]:
    """Filter to soldiers in colliding sub-units."""
    # For 82nd vs 101st collision on regiment 3:
    # - Before: samples from regiments 3,5,7 (82nd) vs 1,3,6 (101st)
    # - After: samples only regiment 3 from both sides
```

Modified `sample_collisions()` (lines 176-214):
- Filters soldiers to colliding sub-units before sampling
- Logs filter reduction (e.g., "300 -> 100 soldiers")
- Falls back to all soldiers with warning if filter returns empty

### 2. Quality Tier Filtering (`llm_phases.py`)

Added `_filter_records_by_quality()` helper (lines 47-125):

```python
def _filter_records_by_quality(
    records: pd.DataFrame,
    mode: str = "vocab",  # or "differentiator"
    max_records: int = 40,
) -> pd.DataFrame:
```

**Modes:**
- `vocab`: Prioritizes tiers 5 → 4 → 3 → 2 → 1 (lowest quality first)
- `differentiator`: Excludes tier 1 entirely, limits tier 2 to 20%, prioritizes 5 → 4 → 3

**Updated functions:**
- `discover_vocabulary()` - uses `mode="vocab"` (lines 457-464)
- `discover_patterns()` - uses `mode="differentiator"` (lines 289-302)
- `mine_exclusions()` - uses `mode="differentiator"` (lines 401-408)

---

## Quality Tier Distribution

Current synthetic data (10K records):
| Tier | Count | % | Description |
|------|-------|---|-------------|
| 1 | 2,094 | 21% | archival_clean |
| 2 | 3,748 | 37% | standard |
| 3 | 2,339 | 23% | field_worn |
| 4 | 1,229 | 12% | degraded |
| 5 | 590 | 6% | fragmentary |

With filtering applied:
| Stage | Mode | Tier 1 | Tier 2 | Tiers 3-5 |
|-------|------|--------|--------|-----------|
| Vocabulary (40 records) | vocab | 0% | 0% | 100% |
| Patterns (20 records) | differentiator | 0% | 0% | 100% |
| Exclusions (30 records) | differentiator | 0% | ≤20% | ≥80% |

---

## Rationale

**Problem:** LLM was producing trivial resolver output (e.g., "Regiment 5 → 82nd Airborne") because:
1. Collision samples included non-overlapping regiments, making disambiguation trivial
2. High-quality records had explicit unit type indicators (e.g., "3rd PIR" instead of just "3rd")

**Solution:** Force LLM to find subtle signals by:
1. Only showing records where designators actually collide
2. Showing primarily degraded/fragmentary records without explicit identifiers

**Expected outcome:** Resolver differentiators should reference vocabulary signals (PIR vs GIR, airborne terminology) rather than unique regiment numbers.

---

## Instruction Status

`002_collision-sampling-synthetic-fix.md`:
- Phase 1 tasks: All complete (4/4)
- Phase 2 (Synthetic Degradation Redesign): Pending - only proceed if Phase 1 validation shows data quality issues

---

## Verification Commands

```bash
# Test collision filter
python3.11 -c "
from src.strategies.resolver.generator.sampling import _filter_to_collision
import pandas as pd
# ... (see inline tests in session)
"

# Test quality filter
python3.11 -c "
from src.strategies.resolver.generator.llm_phases import _filter_records_by_quality
import pandas as pd
raw = pd.read_parquet('data/synthetic/raw.parquet')
vocab = _filter_records_by_quality(raw, mode='vocab', max_records=40)
print(vocab['quality_tier'].value_counts())
"
```

---

## Next Steps

1. Re-run resolver generation for 82nd Airborne Division
2. Verify differentiators reference vocabulary signals, not unique regiments
3. If still producing trivial output, proceed to Phase 2 (Synthetic Degradation Redesign)

---

## Documentation Updates (Session 2)

Updated documentation to reflect Phase 1 completion:

| File | Changes |
|------|---------|
| `docs/components/strategies/resolver/CURRENT.md` | Added collision-scoped sampling and quality tier filtering sections; updated implementation status table to show all modules complete |
| `instructions/active/002_collision-sampling-synthetic-fix.md` | Tasks 1-4 marked complete, status updated to "Phase 1 Complete (pending validation)" |
| `instructions/active/README.md` | Updated instruction table to show Phase 1 Complete |
| `README.md` | Updated Recent section with 2026-01-17 changes |

---

## Related Documents

- Thread extract: `.project_history/extracts/raw/2026-01-17_opus_resolver-collision-sampling.md`
- Instruction: `instructions/active/002_collision-sampling-synthetic-fix.md`

---

## Session 3: Resolver Validation Test

### Summary

Built and ran a validation test for the 82nd Airborne Division resolver. Results confirmed that synthetic data issues (identified in Phase 2 of instruction 002) are the root cause of poor validation metrics.

### Files Created

| File | Purpose |
|------|---------|
| `tests/test_resolver_validation.py` | Validation test for resolver accuracy |
| `instructions/active/003_synthetic-degradation-phase2.md` | Detailed Phase 2 implementation instructions |

### Validation Test Results

Using tier 3+ degraded records (20 soldiers):

| Metric | Accuracy |
|--------|----------|
| Name | 30% (6/20) |
| Regiment | 70% (14/20) |
| Battalion | 45% (9/20) |
| Company | 35% (7/20) |
| **Full Unit** | **25% (5/20)** |

### Issues Identified

1. **Synthetic data too explicit** - Even "degraded" records include unit type indicators (PIR, Inf, Marine)
2. **Company code mismatch** - Ground truth uses letter codes (K, M, E, F) but raw records use numbers (1, 2, 3, 4)
3. **Name extraction limited** - LLM returns abbreviated names from degraded records

### Conclusion

Phase 2 (Synthetic Degradation Redesign) is required before validation can be meaningful. Created detailed instruction `003_synthetic-degradation-phase2.md` with step-by-step implementation guide.

### Instruction Updates

| File | Changes |
|------|---------|
| `instructions/active/003_synthetic-degradation-phase2.md` | Created comprehensive Phase 2 implementation guide |
| `instructions/active/README.md` | Added instruction 003 to current instructions table |

---

## Session 4: Synthetic Data Degradation Phase 2 Implementation

### Summary

Implemented all code changes for synthetic data degradation redesign (instruction 003). This enables generation of realistically ambiguous records that force the resolver to learn vocabulary-based disambiguation.

### Files Modified

| File | Changes |
|------|---------|
| `src/synthetic/models.py` | Added `omit_unit_type: bool = False` to `UnitFormat` dataclass |
| `src/synthetic/renderer.py` | Updated 4 render methods to check `omit_unit_type`: `_render_labeled_hierarchical`, `_render_labeled_full`, `_render_labeled_micro`, `_render_marine_standard` |
| `src/synthetic/clerk_factory.py` | Added parsing of `omit_unit_type` in `_parse_archetype()` and preservation in `_vary_unit_format()` |
| `src/synthetic/source_generator.py` | Rebalanced `QUALITY_TIER_WEIGHTS` (tier 4-5 now 35%), wired `field_minimal` to `ARCHETYPE_BIAS` tier 4-5 |
| `src/synthetic/pipeline.py` | Added `COMPONENT_WEIGHTS_82ND_FOCUSED` (9 components), CLI `--focused-82nd` flag, `component_weights` parameter |
| `docs/.../synthetic_style_spec_v3.yaml` | Added `field_minimal` archetype with `omit_unit_type: true` |
| `synthetic_generation.ipynb` | Created notebook for interactive generation |

### Implementation Details

#### 1. `omit_unit_type` Flag

Added to `UnitFormat` dataclass:
```python
omit_unit_type: bool = False  # When True, renders "3rd" instead of "3rd PIR"
```

Renderer now checks this flag before appending unit type suffixes (PIR, Inf, Marine, etc.).

#### 2. `field_minimal` Archetype

New clerk archetype for maximum ambiguity:
- `omit_unit_type: true` - no type indicators
- `include_division: false` - no division mentions
- `vocabulary_density: low` - minimal extra terms
- Wired to tier 4-5 sources

#### 3. Rebalanced Tier Weights

| Tier | Before | After |
|------|--------|-------|
| 1 (archival) | 20% | 10% |
| 2 (standard) | 35% | 25% |
| 3 (field) | 25% | 30% |
| 4 (degraded) | 15% | 25% |
| 5 (fragment) | 5% | 10% |
| **Tier 4-5** | **20%** | **35%** |

#### 4. Focused Component Weights

Added `COMPONENT_WEIGHTS_82ND_FOCUSED` for targeted resolver validation:
```python
{
    "82nd_airborne_division": 0.20,     # Target
    "101st_airborne_division": 0.12,    # Rival
    "1st_infantry_division": 0.10,      # Rival
    "2nd_infantry_division": 0.10,      # Rival
    "3rd_infantry_division": 0.10,      # Rival
    "36th_infantry_division": 0.08,     # Rival
    "10th_mountain_division": 0.08,     # Rival
    "1st_marine_division": 0.11,        # Rival
    "3rd_marine_division": 0.11,        # Rival
}
```

### Generation Commands

```bash
# CLI with focused weights
./venv/bin/python -m src.synthetic.pipeline --focused-82nd

# Or use notebook
jupyter notebook synthetic_generation.ipynb
```

### Documentation Updates

| File | Changes |
|------|---------|
| `docs/components/synthetic_data_generation/CURRENT.md` | Added v3.0.3 changelog, `field_minimal` archetype, unit type omission section |
| `instructions/active/003_synthetic-degradation-phase2.md` | Marked steps 1-6 complete, updated status to "code-complete (pending validation)" |

### Next Steps

1. Run `synthetic_generation.ipynb` to generate focused dataset
2. Re-run resolver generation for 82nd Airborne Division
3. Re-run validation test
4. Verify acceptance criteria (tier distribution, unit type omission rates)

---

## Session 5: Performance Optimizations & Notebook Improvements

### Summary

Fixed critical timeout issues causing resolver generation to hang indefinitely, optimized package imports for faster notebook loading, and added progress tracking for better visibility.

### Files Modified

| File | Changes |
|------|---------|
| `src/utils/llm/providers/gemini.py` | Fixed timeout configuration, removed invalid httpx.Client usage |
| `src/strategies/resolver/__init__.py` | Converted to lazy imports via `__getattr__` |
| `src/strategies/resolver/generator/__init__.py` | Converted to lazy imports for heavy modules |
| `src/strategies/resolver/generator/generate.py` | Added progress_callback parameter, reduced max_retries from 3 to 1, timeout 300s |
| `src/strategies/resolver/generator/llm_phases.py` | Added progress_callback parameter to run_all_phases |
| `resolver_generation.ipynb` | Added tqdm progress bar, lazy imports in cells |
| `docs/components/strategies/resolver/CURRENT.md` | Added "Performance Optimizations" section |

### Implementation Details

#### 1. Timeout Configuration Fix

**Problem:** Gemini API calls timing out after 30-45 minutes instead of configured 2 minutes.

**Root Cause:**
- Passing custom `httpx.Client` object caused Pydantic validation error
- `ChatGoogleGenerativeAI` doesn't accept `client` parameter

**Solution:**
```python
# Before (broken):
http_client = httpx.Client(timeout=httpx.Timeout(timeout=timeout_seconds))
model_kwargs = {"client": http_client}  # ValidationError!

# After (working):
model_kwargs = {
    "timeout": timeout_seconds,  # Pass directly
}
```

**Configuration:**
- Default timeout: 300s (5 minutes) - up from 120s
- Max retries: 1 - down from 3
- Location: `src/utils/llm/providers/gemini.py:44-54`

**Impact:**
- Max time per call: 10 minutes (5 min × 2 attempts)
- Failed calls surface in 10 min instead of 2+ hours

#### 2. Lazy Import Optimization

**Problem:** Importing from `src.strategies.resolver.generator` took 5-10 seconds due to eager loading of LangChain.

**Root Cause:**
```python
# Old __init__.py (eager):
from .executor.strategy import ResolverStrategy  # Loads LangChain immediately
from .generate import generate_all_resolvers     # Loads LLM providers
```

**Solution:**
```python
# New __init__.py (lazy):
def __getattr__(name):
    if name == "ResolverStrategy":
        from .executor.strategy import ResolverStrategy
        return ResolverStrategy
    # ... other imports
```

**Changes:**
- `src/strategies/resolver/__init__.py` - All imports lazy
- `src/strategies/resolver/generator/__init__.py` - Heavy modules (llm_phases, generate) lazy, lightweight (thresholds, structure, sampling) eager

**Impact:**
- Import time: 5-10s → 0.5-1s for lightweight functions
- Notebook cell-2 now runs instantly
- LangChain only loads when calling generation functions

#### 3. Progress Callback Support

**Added Parameter:**
```python
def generate_single_component(
    component_id: str,
    ...
    progress_callback: Optional[callable] = None,
) -> Dict[str, Any]:
```

**Callback Invocations:**
- "Pattern Discovery"
- "Exclusion Mining"
- "Vocabulary Discovery"
- "Differentiator Generation"
- "Tier Assignment"
- "Complete"

**Notebook Integration:**
```python
from tqdm.auto import tqdm

pbar = tqdm(total=6, desc=f"Generating {COMPONENT_ID}", unit="phase")

def update_progress(phase_name: str):
    pbar.set_postfix_str(phase_name)
    pbar.update(1)

resolver = generate_single_component(
    component_id=COMPONENT_ID,
    progress_callback=update_progress,
    ...
)
```

**Impact:**
- Real-time visibility into current phase
- Progress percentage and time estimates
- Easier to identify stuck phases

#### 4. Retry Configuration

**Change:** `RetryConfig(max_retries=1)` down from default 3

**Rationale:**
- 5 min timeout × 4 attempts = 20 min per failing call
- Differentiator generation has 9 rivals
- One bad call could waste hours with 3 retries
- Better to fail fast and surface errors

**Location:** `src/strategies/resolver/generator/generate.py:243-249`

### Documentation Updates

| File | Changes |
|------|---------|
| `docs/components/strategies/resolver/CURRENT.md` | Added "Performance Optimizations (2026-01-17)" section before References |
| `.project_history/code-activity/2026-01-17.md` | Added Session 5 entry (this section) |

### Verification

**Test timeout fix:**
```bash
# Should complete or timeout in 5 min, not hang indefinitely
python3.11 -c "
from src.utils.llm import create_provider
llm = create_provider('gemini-2.5-pro', timeout=10)
# Test call...
"
```

**Test lazy imports:**
```bash
# Should be fast (~1s)
time python3.11 -c "from src.strategies.resolver.generator import ThresholdResult"

# Should be slower (loads LangChain)
time python3.11 -c "from src.strategies.resolver.generator import generate_single_component"
```

**Test progress bar:**
Run `resolver_generation.ipynb` cell-8 and observe tqdm progress

### Related Sessions

- Session 1: Collision sampling fix
- Session 2: Documentation updates
- Session 3: Resolver validation test
- Session 4: Synthetic degradation redesign
